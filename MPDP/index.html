<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <!--
    <script src="./resources/jsapi" type="text/javascript"></script>
    <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
   -->
  
  <style type="text/css">
    @font-face {
     font-family: 'Avenir Book';
     src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
     }
  
    body {
      font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight:300;
      font-size:14.7px;
      margin-left: auto;
      margin-right: auto;
      width: 800px;
    }
    h1 {
      font-weight:300;
    }
    h2 {
      font-weight:300;
    }
    h3 {
      font-weight:270;
    }
  
    p {
      font-weight:300;
      line-height: 1.4;
    }
  
    code {
      font-size: 0.8rem;
      margin: 0 0.2rem;
      padding: 0.5rem 0.8rem;
      white-space: nowrap;
      background: #efefef;
      border: 1px solid #d3d3d3;
      color: #000000;
      border-radius: 3px;
    }
  
    pre > code {
      display: block;
      white-space: pre;
      line-height: 1.5;
      padding: 0;
      margin: 0;
    }
  
    pre.prettyprint > code {
      border: none;
    }
  
  
    .container {
          display: flex;
          align-items: center;
          justify-content: center
    }
    .image {
          flex-basis: 40%
    }
    .text {
          padding-left: 20px;
          padding-right: 20px;
    }
  
    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      padding: 20px;
    }
  
    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
    }
  
    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
    }
  
    img.rounded {
      border: 0px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
  
    }
  
    a:link,a:visited
    {
      color: #1367a7;
      text-decoration: none;
    }
    a:hover {
      color: #208799;
    }
  
    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }
  
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
              0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
              5px 5px 0 0px #fff, /* The second layer */
              5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
              10px 10px 0 0px #fff, /* The third layer */
              10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
              15px 15px 0 0px #fff, /* The fourth layer */
              15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
              20px 20px 0 0px #fff, /* The fifth layer */
              20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
              25px 25px 0 0px #fff, /* The fifth layer */
              25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }
  
  
    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
              0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
              5px 5px 0 0px #fff, /* The second layer */
              5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
              10px 10px 0 0px #fff, /* The third layer */
              10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }
  
    .vert-cent {
      position: relative;
        top: 50%;
        transform: translateY(-50%);
    }
  
    hr
    {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
  </style>
  
    <title>Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning</title>
  </head>
  
  <body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
    <br>
    <center>
    <span style="font-size:36px">Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning</span><br><br><br>
    </center>
    <table align="center" width="850px">
              <tbody><tr>
                      <td align="center" width="180px">
                <center>
                  <span style="font-size:15.5px"><a href="https://scholar.google.com.sg/citations?user=ZfB7vEcAAAAJ&hl=en">Chuming Li</a><sup>* 1,2</sup></span>
                  </center>
                  </td>
                      <td align="center" width="180px">
                <center>
                  <span style="font-size:15.5px"><a href="https://jiaruonan.github.io/">Ruonan Jia</a><sup>* 1,3</sup></span>
                  </center>
                </td>
                      <td align="center" width="100px">
                <center>
                  <span style="font-size:15.5px"><a href="https://scholar.google.com.sg/citations?user=c--XCzAAAAAJ&hl=en">Jie Liu</a><sup>1</sup></span>
                  </center>
                </td>
                      <td align="center" width="180px">
                <center>
                  <span style="font-size:15.5px"><a href="https://yinminzhang.github.io/">Yinmin Zhang</a><sup>1,2</sup></span>
                  </center>
                  </td>
                      <td align="center" width="170px">
                <center>
                  <span style="font-size:15.5px"><a href="https://github.com/PaParaZz1">Yazhe Niu</a><sup>1</sup></span>
                  </center>
                  </td>
                      <td align="center" width="170px">
                <center>
                  <span style="font-size:15.5px"><a href="https://www.yangyaodong.com/">Yaodong Yang</a><sup>4</sup></span>
                  </center>
                  </td>
                      <td align="center" width="100px">
                <center>
                <span style="font-size:15.5px"><a href="https://liuyu.us/">Yu Liu</a><sup>1</sup></span>
                  </center>
                  </td>
                      <td align="center" width="180px">
                <center>
                <span style="font-size:15.5px"><a href="https://wlouyang.github.io/">Wanli Ouyang</a><sup>1</sup></span>
                  </center>
              
            </td></tr>
          </tbody></table><br>
    
      <table align="center" width="700px">
              <tbody><tr>
                <td align="center" width="50px">
                <center>
                      <span style="font-size:16px"></span>
                </center>
                </td>

                <td align="center" width="300px">
                <center>
                      <span style="font-size:16px"><sup>1</sup>Shanghai AI Lab</span>
                </center>
                </td>
                      
                <td align="center" width="300px">
                <center>
                      <span style="font-size:16px"><sup>2</sup>University of Sydney</span>
                </center>
                </td>

                <td align="center" width="300px">
                <center>
                    <span style="font-size:16px"><sup>3</sup>Tsinghua University</span>
                </center>
                </td>                  
                  
                <td align="center" width="300px">
                <center>
                    <span style="font-size:16px"><sup>4</sup>Peking University</span>
                </center>
                </td>      

                <!-- <td align="center" width="300px">
                <center>
                    <span style="font-size:16px"><sup>5</sup>SenseTime Research</span>
                </center>
                </td>       -->
          </tr></tbody></table>
  
     <br>
        <table align=center width=500px>
          <tr>
            <td align=center width=500px>
              <center>
                <span style="font-size:22px">
                  <span style="font-size:20px">ECAI 2023</span>
                </span>
              </center>
            </td>
          </tr>
        </table> 
    <br><hr>
    <table align="center" width="700px">
              <tbody><tr>
                <!-- <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">Code
                      <a href="https://github.com/Lipurple/Grounded-Diffusion"> [GitHub]</a>
                    </span>
                  </center>
                </td> -->
  
                <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">
                      Paper <a href="https://arxiv.org/abs/2307.12933"> [arXiv]</a>
                    </span>
                  </center>
                </td>
  
                <td align="center" width="200px">
                  <center>
                    <br>
                    <span style="font-size:20px">
                      Cite <a href="./resources/bibtex.txt"> [BibTeX]</a>
                    </span>
                  </center>
                </td>
              </tr></tbody>
        </table>
    
        <br><hr>
       
  
        <center><h2> Abstract </h2> </center>
        <p style="text-align:justify; text-justify:inter-ideograph;">
        </p><div class="container">
          <div class="text" width="400px"> 
            <p style="text-align:justify; text-justify:inter-ideograph;">
              <left>
                Model-based reinforcement learning (RL) has demonstrated remarkable successes on a range of continuous control tasks due to its high sample efficiency. To save the computation cost of conducting planning online, recent practices tend to distill optimized action sequences into an RL policy during the training phase. Although the distillation can incorporate both the foresight of planning and the exploration ability of RL policies, the theoretical understanding of these methods is yet unclear. In this paper, we extend the policy improvement of Soft Actor-Critic (SAC) by developing an approach to distill from model-based planning to the policy. We then demonstrate that such an approach of policy improvement has a theoretical guarantee of monotonic improvement and convergence to the maximum value defined in SAC. We discuss effective design choices and implement our theory as a practical algorithm—<i><b>M</b>odel-based <b>P</b>lanning <b>D</b>istilled to <b>P</b>olicy (<b>MPDP</b>)</i>—that updates the policy jointly over multiple future time steps. Extensive experiments show that MPDP achieves better sample efficiency and asymptotic performance than both model-free and model-based planning algorithms on six continuous control benchmark tasks in MuJoCo.

            </left></p>
          </div>
        </div>
        <br>
        <hr>
        <br>


        <center> <h2> Overview </h2> </center>
        <h3> Previous Approach </h3>
        <ul>
            <li>First, prior works in model-based planning can hardly be applied in realtime, because these need to solve an optimization problem on each time step and cannot remember the solution for reuse in the future similar states.<br></li>
            <li>Second, these methods only optimize the maximum of the reward sum over the future states, rather than the trade-off between exploration and exploitation, which limits the ability to discover diverse states and better policies.<br></li>
          </ul>

        A thorough component comparison of relevant algorithms is given in the following table.

        <p><img class="center" src="./resources/features.png" width="800px"></p>

        <h3> Our Approach(MPDP) </h3>
        <p style="text-align:justify; text-justify:inter-ideograph;"><left>
            We propose a model-based extended policy improvement method, which utilizes model-based planning to distill RL policy and model regularization to reduce the model errors. 
            We also demonstrate that our method has a theoretical guarantee of monotonic improvement and convergence.</br>

           </left></p>
        <br>
        <hr>
        

        <center><h2> Method </h2></center>
        <p><left>
          We conclude our extended policy improvement in Algorithm 1. The algorithm processes a batch of states at each iteration and the model rollouts states until the task terminates. To separate the policy on each time step, we maintain the policy networks at H time steps. The policy networks generate the actions for each step and are updated jointly in our extended improvement step. After the rollout, the policy networks are updated with the gradients to the action sequence. </br>
          </br>
          The complete algorithm is described in Algorithm 2. The method alternates among using the initial policy on the first step to interact with the environment, training an ensemble of models, and updating the policy with policy evaluation and our extended policy improvement from batches sampled from the replay buffer.
        </left></p>
        <p><img class="center" src="./resources/algorithm.png" width="810px"></p>
        <br>
        <hr>


        <center><h2> Lemma and Theorem </h2></center>
        <p><left>
          We propose an approach to distilling the solution of model-based planning into the policy, which is a multi-step extension of policy improvement of original SAC. We verify its theoretical properties. See Appendix for more details.
        </left></p>
        <p><img class="center" src="./resources/theorems.png" width="810px"></p>
        <br>
        <hr>
  
        <center><h2> Results </h2></center>
        <h3> Comparison with the previous model-based and model-free methods on MuJoCo-v2 </h3>
        <p><left>
            Performance curves for our method (MPDP) and baselines on MuJoCo continuous control benchmarks. Solid lines depict the mean of four random seeds and shaded regions correspond to standard deviation among seeds. The dashed lines indicate the asymptotic performance of PETS at the corresponding training steps (15k steps for InvertedPendulum, 100k steps for Hopper, and 200k steps for the other tasks) and SAC at 2M steps.
        </left></p>
        <p><img class="center" src="./resources/result.png" width="800px"></p>

        <!-- <p><b>Visualization of environments.</b> </p>
        <p><left>
            We visualize the six continuous control tasks in MuJoCo-v2 including InvertedPendulum, Hopper, HalfCheetah, Ant, Walker2d, and Humanoid. The first task InvertedPendulum is designed to control the pole to keep balance, and the other five tasks aim to keep the agent moving forward without falling.
        </left></p>
        <p><img class="center" src="./resources/envs.png" width="800px"></p> -->
        <br>
        <hr>
  
        
        <center><h2>Ablation Study</h2></center>
        <h3> Effect on the designed adaptive horizon </h3>
        <div class="container">
          <div class="image" width="550px">
            <center><p><img class="center" src="./resources/horizon_length.png" width="350px"></p></center>
          </div>
          <div class="text" width="250px"> 
            <p> This figure demonstrates the length of the adaptive horizon of MPDP. The solid lines denote the average horizon length evaluated on each training batch. As the interactions accumulate, the model generalizes better and our method rapidly adapts to longer horizons.
            </p>
          </div>
        </div>

        <h3> Effect of the regularization on the model error </h3>
        <div class="container">
          <div class="image" width="550px">
            <center><p><img class="center" src="./resources/beta_error.png" width="352px"></p></center>
          </div>
          <div class="text" width="250px"> 
            <p> This figure shows the model error curves of MPDP with β varying from 0.2 to 0.7, measured by the average L2 norm of the predicted states on every 250 interactions. The model error decreases with β, which verifies that optimizing under our regularization effectively restricts behavior policy in the areas with low model error.
            </p>
          </div>
        </div>

        <div class="container">
          <div class="image" width="550px">
            <center><p><img class="center" src="./resources/beta_perform.png" width="355px"></p></center>
          </div>
          <div class="text" width="250px"> 
            <p> This figure shows the performance of MPDP with β varying from 0.2 to 0.7 along with MBPO on the Hopper task, evaluated over 4 trials. As β increases, the performance increases at first then decreases due to too strong restriction on the exploration. It also reveals that a larger regularization achieves more robust results.
            </p>
          </div>
        </div>
        <br>
        <hr>


        <center> <h2> Cite This Paper </h2> </center>
        <div class="container">
            <pre><code>
                @inproceedings{li2023mpdp,
                    author = {Chuming Li, Ruonan Jia, Jie Liu, Yinmin Zhang, Yazhe Niu, Yaodong Yang, Yu Liu, Wanli Ouyang},
                    title = {Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning},
                    journal = {arXiv:2307.12933},
                    year = {2023}
                }
            </code></pre>
        </div>
        <br>
        <hr>

        <center> <h2> Acknowledgements </h2> </center>
        <p> 
          Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
        </p>
        <br>
  <br>
  

  </body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
  